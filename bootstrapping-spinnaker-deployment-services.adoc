= Bootstrapping Spinnaker Deployment Services                                                                                
Dan Woods <danw@netflix.com>
2014-09-06
:toc: right
:toclevels: 4
:sectanchors:
:sectlink:
:linkattrs:
:numbered:
:appversion: 1.0-SNAPSHOT
:source-highlighter: prettify

== Introduction

Spinnaker's deployment facet consists of three services, an orchestration engine, and a UI. This slice of the platform is enabled to operate independently from the continuous delivery components. In that respect, continuous deployment can be facilitated using **only** the deployment-related services. In effect, these services represent the capabilities previously offered by Asgard.

The three core services of Spinnaker's deployment aspects are its deployment engine, its cluster service, and its infrastructure service. These three services provide the capabilities to read from and write to the cloud. This document will cover building these projects, configuring them, and running them. Spinnaker's deployment orchestration engine is a service that provides task execution state to send operations to the deployment engine and monitor the effects from the cluster service and infrastructure service. The orchestration engine in this vertical of the architecture is a lightweight version of Spinnaker's continuous delivery orchestration engine, and exposes its deployment capabilities through a simplistic REST API. The deployment user interface is a trimmed down version of Spinnaker's unified user interface, and it focuses on deployment related activities and insight into clusters.

This document will focus on bootstrapping this slice of the architecture for AWS support.

[[KATO]]
== Bootstrapping the Deployment Engine

Spinnaker's deployment engine, Kato, provides all write-to operations to the cloud for deployment and deployment-related activities. It is designed strictly to facilitate atomic operations that can be invoked in a synchronous way, without maintaining any state (outside of in-flight operations) within the service itself. In this respect, Kato is designed to facilitate tasks that don't require any higher-level orchestration. Internally, Kato has a primitive orchestration engine, but this should not be confused with **deployment orchestration**. Kato's orchestration engine is simply to tie together sets of atomic operations to provide "bulk" and "chained" processing. Atomic operations can be chained together to affect higher-order deployment goals, like building a load balancer, creating a DNS record, and deploying an AutoScaling group. Kato's documentation covers its capabilities in detail.

=== Building Kato

Following a fresh checkout of Kato from source control, the project can be built using the provided Gradle wrapper. Java 7 or higher is required to build the project. There is a known bug with Java 7u65 and Java 8u11 that affect Kato, so as of the time of this writing, Java 7u60 is the recommended JDK version. The project can be built simply by invoking the +build+ task, in the form of `./gradlew build`, which will compile the project and all submodules. The Spring Boot repackaging task, which generates the 'fat jar' is presently disabled, so generating a distribution needs to occur through the +distTar+ or +distZip+ task. The resulting artifact of consequence can be found in the +kato-web/build/distributions+ directory. 

NOTE: Kato's web module is what ties the submodules together and provides the RESTful API and the project's +Main+ class. 

=== Running Kato

From a development position, Kato can be started either through invoking the +bootRun+ task on the +kato-web+ module, or through an IDE by creating a Java Application run configuration for the +com.netflix.spinnaker.kato.Main+ class, which resides in the +kato-web+ module. There is no preference to either approach; the choice is the discretion of the developer.

For Kato to run in a deployment, a distribution artifact must be generated, as outlined in the prior section. Once the +kato-web+ tarball or zip file is generated, it can be copied to a server and extracted. From the extracted contents, there exists a +bin+ directory, and within it a +kato-web+ executable script, which can be used to start Kato on the server. To visualize this, consider that the Kato +distTar+ task generated a tarball named +kato-web-1.3.6-SNAPSHOT.tar+. If you copied this to the +/opt+ directory of a server named +kato+ using the command +scp kato-web-1.3.6-SNAPSHOT.tar user@kato:/opt+, then SSH'd to the +kato+ server and cd'd to the +/opt+ directory and executed +tar xvf kato-web-1.3.6-SNAPSHOT.tar+, you'd have a resulting directory +/opt/kato-web-1.3.6-SNAPSHOT+. Within that directory would be a +bin+ subdirectory, and therein the +kato-web+ executable. Executing +./kato-web+ will start Kato from the command line.

NOTE: Once Kato is deployed, its API and manual is accessible through the URI +/manual/index.html+.

=== Configuring Kato

Basic AWS configuration is required to have Kato be able to affect changes for a given account at Amazon, which is covered in detail in the <<CONF>> section of this document.

Kato's additional configuration can be supplied as either Java properties file, YAML configuration file, or as system properties to the JVM. If the former two options are chosen, their location can be made known to Kato by supplying their location as part of the +spring.config.locations+ system property. For example, if you wanted Kato to resolve its configuration from +/etc/kato/system.yml+, you should pass the +-Dspring.config.locations=/etc/kato/system.yml+ system property to the JVM on startup. This can be accomplished by exporting the +JAVA_OPTS+ variable as part of starting the +kato-web+ executable, in the form of: +JAVA_OPTS='-Dspring.config.locations=/etc/kato/system.yml' ./kato-web+. Alternatively, the system property can be placed directly into the +kato-web+ startup script as part of the +JVM_OPTS+ variable at the beginning of the script. The +kato-web+ buildscript (+kato-web.gradle+) could also be modified to include the `applicationDefaultJvmArgs = ["-Dspring.config.locations=/etc/kato/system.yml"]` directive, which would allow the distribution to be packaged automatically with these parameters.

Default configurations for +iamRole+ and +keyPair+ can be overridden for your specific environment. The +iamRole+ configuration describes what http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html[IAM Role] new launch configurations should be shipped with. The +keyPair+ directive can be configured to default new deployments to a specific http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html[AWS EC2 Key Pair], which is necessary to access your servers via ssh.

These values can be overridden in the following form of a YAML configuration:

```
aws:
  defaults:
    iamRole: BaseIAMRole
    keyPair: my-keypair
```

These values may also be overridden through Java system properties through dot notation. For example, to override the default +iamRole+ directive, you should pass the system property: +-Daws.defaults.iamRole=OtherIAMRole+; likewise for the +keyPair+ configuration.

[[OORT]]
== Bootstrapping the Cluster Service

Spinnaker's cluster service, Oort, provides a detailed view into an application's presence in the cloud. Its RESTful API follows a resource-oriented structure, with the top-level element collating clusters by application. In Spinnaker's view of the cloud, applications have many clusters, clusters have many server groups and load balancers, and server groups have many instances. Within AWS, these relationships are realized through a common nomenclature of AutoScaling groups (server groups) and load balancers; since AWS does not have any concept of clusters, they are known to Spinnaker only through the naming conventions followed for AutoScaling groups. In that respect, they are also transient objects to the cluster server: without at least one AutoScaling group for a cluster, the cluster does not exist. Additionally, applications can also exist as transients, and when one AutoScaling group with the appropriate naming convention comes into existence, the application will be known to Oort as well. Oort has the capability to interface with Spinnaker's Application Metadata Service, Front50, to allow applications to be persistent in Oort's graph.

The naming convention is defined through +frigga+, which is a library used throughout Spinnaker (and Netflix) to ascertain the concepts of application, cluster, and stack from AutoScaling group, load balancer, or security group names. The scheme is defined as such:

  * **application-stack-v000--extraDetails**
    - Application: `application`
    - Cluster: `application-stack`
    - Stack: `stack`
    - Push Sequence: `v000`
    - Free-form Detail: `extraDetails`

Stack is not strictly required, and without it, the cluster will be named the same as the application. The concept of a stack gives developers the ability to differentiate their target deployment environments. For example, +test+, +int+, and +prod+ are all good examples for stack. The push sequence is determined by Spinnaker's deployment engine (<<KATO>>), and acts as a way of determining ancestry within a cluster. The free form details provided serve as a point for developers to be able to visually determine if the AutoScaling group they are looking at is their intended subject. For example, free form detail may be used to indicate internal/external subnets, or may inform as to the availability zone of a particular AutoScaling group.

Oort's infrastructure caches pertinent details of the cloud, and uses that cache to construct the data model exposed by its API. To that extent, details about the cloud may be delayed up to the interval for which the caching agents run. Given its caching infrastructure, Oort is able to quickly search its indexes for cloud details that it knows about, and it provides a comprehensive search API for consuming services. The details of Oort's API are covered in depth on its https://github.com/spinnaker/oort/wiki[Github Wiki Page].

=== Building Oort

Following a fresh checkout of Oort from source control, the project can be built using the provided Gradle wrapper. Java 7 or higher is required to build the project. There is a known bug with Java 7u65 and Java 8u11 that affect Oort, so as of the time of this writing, Java 7u60 is the recommended JDK version. The project can be built simply by invoking the +build+ task, in the form of `./gradlew build`, which will compile the project and all submodules. The Spring Boot repackaging task, which generates the 'fat jar' is presently disabled, so generating a distribution needs to occur through the +distTar+ or +distZip+ task. The resulting artifact of consequence can be found in the +oort-web/build/distributions+ directory.

=== Running Oort

From a development position, Oort can be started either through invoking the +bootRun+ task on the +oort-web+ module, or through an IDE by creating a Java Application run configuration for the +com.netflix.spinnaker.oort.Main+ class, which resides in the +oort-web+ module. There is no preference to either approach; the choice is the discretion of the developer.

For Oort to run in a deployment, a distribution artifact must be generated, as outlined in the prior section. Once the +oort-web+ tarball or zip file is generated, it can be copied to a server and extracted. From the extracted contents, there exists a +bin+ directory, and within it an +oort-web+ executable script, which can be used to start Oort on the server. To visualize this, consider that the Oort +distTar+ task generated a tarball named +oort-web-0.11-SNAPSHOT.tar+. If you copied this to the +/opt+ directory of a server named +oort+ using the command +scp oort-web-0.11-SNAPSHOT.tar user@oort:/opt+, then SSH'd to the +oort+ server, cd'd to the +/opt+ directory, and executed +tar xvf oort-web-0.11-SNAPSHOT.tar+, you'd have a resulting directory +/opt/oort-web-0.11-SNAPSHOT+. Within that directory would be a +bin+ subdirectory, and therein the +oort-web+ executable. Executing +./oort-web+ will start Oort from the command line.

Oort's caching agents will start automatically, but it's important to note that the API will not block until a successful read has completed. In that respect, the details coming out of Oort from a fresh start will become "eventually consistent".

=== Configuring Oort

Basic AWS configuration is required to have Oort be able to affect changes for a given account at Amazon, which is covered in detail in the <<CONF>> section of this document.

[[MORT]]
== Bootstrapping the Infrastructure Service

Spinnaker's infrastructure service, Mort, is responsible for providing infrastructure related data about the cloud, and providing a view into that data through a resource-oriented API. In terms of AWS, "infrastructure" means VPCs, subnets, security groups, load balancers, and accounts. 

Even though load balancers are technically part of the cluster model, they may not be strictly associated (or associable) with a cluster, and therefore they need to be curated as top-level objects in the terms of "infrastructure". Spinnaker's cluster service (<<OORT>>) will provide a reference to the load balancer, and details of it can be accessed through Mort. Consumers that need to know about load balancers outside of the context of a cluster can utilize Mort to gain insight into those details.

Details of security groups follow the same methodology of load balancers, whereby they may be associated with a particular application, cluster, load balancer, or server group, but they do not strictly **need** to be. In that respect, the cluster service will provide references to security groups, but the details are made accessible through Mort.

Subnets are an important aspect of Spinnaker's deployment process. They are encapsulated in the abstraction of a "purpose", which can be specified during a deployment to ascertain the availability zones for an AutoScaling group. This "purpose" is encoded in a tag on the subnet with a key of "immutable_metadata", and is a JSON blob that informs Spinnaker's deployment engine as to the purpose and target for a subnet, ensuring that it chooses the proper destination during launch configuration creation. An example of the JSON data stored in this tag is: +{ "purpose": "internal", "target": "elb" }+. This detail informs the deployment engine that the particular subnet is appropriate for load balancers that wish to be accessible ++only++ internally. Valid options for +target+ are +elb+ and +ec2+, where the latter indicates that AutoScaling groups can be created in this target. Purposes can be generally arbitrarily named, with the exception of the "internal" purpose, which informs AWS of a special internal networking case.

Like all Spinnaker services, Mort is capable of curating data across any number of configured accounts, but in the respect of Spinnaker as a platform, Mort should be known as the canonical source for account configuration and detail. Accounts fit into the paradigm of "infrastructure", and consumers needing to know, for example, what accounts Spinnaker services are aware of, should use Mort as the source of truth. It should be ensured that the deployment engine and cluster service account configurations are synchronized with Mort's.

=== Building Mort

Following a fresh checkout of Mort from source control, the project can be built using the provided Gradle wrapper. Java 7 or higher is required to build the project. There is a known bug with Java 7u65 and Java 8u11 that affect Mort, so as of the time of this writing, Java 7u60 is the recommended JDK version. The project can be built simply by invoking the +build+ task, in the form of `./gradlew build`, which will compile the project and all submodules. The Spring Boot repackaging task, which generates the 'fat jar' is presently disabled, so generating a distribution needs to occur through the +distTar+ or +distZip+ task. The resulting artifact of consequence can be found in the +mort-web/build/distributions+ directory.

=== Running Mort

From a development position, Mort can be started either through invoking the +bootRun+ task on the +mort-web+ module, or through an IDE by creating a Java Application run configuration for the +com.netflix.spinnaker.mort.Main+ class, which resides in the +mort-web+ module. There is no preference to either approach; the choice is the discretion of the developer.

For Mort to run in a deployment, a distribution artifact must be generated, as outlined in the prior section. Once the +mort-web+ tarball or zip file is generated, it can be copied to a server and extracted. From the extracted contents, there exists a +bin+ directory, and within it a +mort-web+ executable script, which can be used to start Mort on the server. To visualize this, consider that the Mort +distTar+ task generated a tarball named +mort-web-0.11-SNAPSHOT.tar+. If you copied this to the +/opt+ directory of a server named +mort+ using the command +scp mort-web-0.11-SNAPSHOT.tar user@mort:/opt+, then SSH'd to the +mort+ server, cd'd to the +/opt+ directory, and executed +tar xvf mort-web-0.11-SNAPSHOT.tar+, you'd have a resulting directory +/opt/mort-web-0.11-SNAPSHOT+. Within that directory would be a +bin+ subdirectory, and therein the +mort-web+ executable. Executing +./mort-web+ will start Mort from the command line.

Mort's caching agents will start automatically, but it's important to note that the API will not block until a successful read has completed. In that respect, the details coming out of Mort from a fresh start will become "eventually consistent".

=== Configuring Mort

Basic AWS configuration is required to have Mort be able to affect changes for a given account at Amazon, which is covered in detail in the <<CONF>> section of this document.

== Bootstrapping the Deployment Orchestration Engine

At the core of Spinnaker's broader platform exists a robust orchestration engine, which is responsible for managing, monitoring, and invoking continuous delivery pipeline configurations. It is in this engine that long running tasks and observations of the cloud can be made to automatically affect some change based on configuration. This aspect of Spinnaker, affectionated known as Orca, is the mechanism by which continuous delivery and deployments are achieved. It utilizes the Spinnaker's entire suite of services to accomplish its goals, and in that makes it is the central figure of the platform. It's capabilities, however, can be utilized in a library form to achieve a much less robust, though still powerful, faculty of the Spinnaker ecosystem. Given that, Spinnaker's deployment facet is able to utilize Orca's capabilities to manage a task infrastructure for running orchestrated deployments and deployment related activities through a wrapper service known as Pond, which exposes Orca's capabilities through a simplistic RESTful API and in-memory task repository.

The need to orchestrate deployments stems from the "eventually consistent" nature of the cloud. Consider an example of creating a new AutoScaling group: from a service standpoint, a call can be made to Spinnaker's deployment engine, which will construct the appropriate prerequisites (launch configuration, allow launch configurations), and enact the creation of the autoscaling group. There's no guarantee, however, as to when this operation will complete, and AWS's criteria for success are generally different than those in a continuous delivery and deployment environment. To better explain that, it's important to understand that from AWS's perspective, an instance in an AutoScaling group is "healthy" once it has been scheduled and moved to a "running" state -- effectively the VM has launched. From a continuous delivery perspective, this is not the case; an instance may not be healthy until its health checks have cleared in the ELB, for example, which is a much more useful metric than "the server has been powered up". Spinnaker's orchestration engine provides the composition of activities to get to a point where a deployment or related activity has reached some judgement of success.

To understand this better, consider the most simple case of a deployment. After an AMI has been staged, a deployment can take place by making a call to the deployment engine, then polling its resource endpoint with the cluster service until some number of instances are reported as healthy. For consumers of Spinnaker's deployment services, it may be cumbersome to build or script mechanisms that perform this disjointed operation. Instead, they can make employ Pond, which is designed for composing such operations into a single, orhcestrated operation. Consumers then need only check back with Pond periodically to know when the operation has succeeded. Pond houses all of the higher-level business logic associated with the orchestration of deployment related activities. Another example demonstrating orchestrating is the resizing of an AutoScaling group. Sending a call to the deployment engine to resize an AutoScaling group will invoke the call with AWS, and AWS may act very quickly to reflect the new capacity, however this operation may not be considered completed until the *actual* number of instances has catched the new desired capacity. Pond's role in the orchestration of these tasks is critical to resulting behavior of Spinnaker's deployment aspects.

[[BUILD-POND]]
=== Building Pond

Following a fresh checkout of Port from source control, the project can be built using the provided Gradle wrapper. Java 7 or higher is required to build the project. There is a known bug with Java 7u65 and Java 8u11 that affect Mort, so as of the time of this writing, Java 7u60 is the recommended JDK version. The project can be built simply by invoking the +build+ task, in the form of `./gradlew build`, which will compile the project. The Spring Boot repackaging task, which generates the 'fat jar' is presently disabled, so generating a distribution needs to occur through the +distTar+ or +distZip+ task. The resulting artifact of consequence can be found in the +build/distributions+ directory.

If the URLs for Kato and Oort are known at Pond's build time (for example, if they're behind an ELB with DNS), they can be specified as JVM system properties to the Gradle build. This will encode the URLs into the distribution executable, which means that no runtime configuration is necessary (although can still be utilized) to get to Kato and Oort. The default values for Kato and Oort are +http://localhost:8501+ and +http://localhost:8080+ respectively. To override these values, the build script can be executed as such: +GRADLE_OPTS='-Dkato.url=http://kato.url:port -Doort.url=http://oort.url:port' ./gradlew clean build distTar+.

=== Running Pond

From a development position, Pond can be started either through invoking the +bootRun+ task or through an IDE by creating a Java Application run configuration for the +com.netflix.spinnaker.pond.Main+ class. There is no preference to either approach; the choice is the discretion of the developer.

For Pond to run in a deployment, a distribution artifact must be generated, as outlined in the prior section. Once the +pond+ tarball or zip file is generated, it can be copied to a server and extracted. From the extracted contents, there exists a +bin+ directory, and within it a +pond+ executable script, which can be used to start Pond on the server. To visualize this, consider that the Pond +distTar+ task generated a tarball named +pond-0.1-SNAPSHOT.tar+. If you copied this to the +/opt+ directory of a server named +pond+ using the command +scp pond-0.1-SNAPSHOT.tar user@pond:/opt+, then SSH'd to the +pond+ server, cd'd to the +/opt+ directory, and executed +tar xvf pond-0.1-SNAPSHOT.tar+, you'd have a resulting directory +/opt/pond-0.1-SNAPSHOT+. Within that directory would be a +bin+ subdirectory, and therein the +pond+ executable. Executing +./pond+ will start Pond from the command line.

=== Configuring Pond

To orchestrate deployments and deployment related activities, Pond needs to know how to reach Spinnaker's deployment and cluster services -- Kato and Oort respectively. As described in the <<BUILD-POND>> section, this can be accomplished at build time. However, if the values are unknown at build time, they can be specified during the invocation of the Pond executable. The values can be configured in a Java properties file, a YAML configuration file, or as JVM system properties during application startup. For the former two options, the location of the configuration file must be specified as a system property under the +spring.config.locations+ key. For example, if you wanted to specify the configuration in a file named +/etc/pond/system.yml+, then you would need to pass those values to the Pond executable in the form of +JAVA_OPTS='-Dspring.config.locations=/etc/pond/system.yml' ./pond+.

The configuration of these directives actually falls back to Orca, but we can pass them through Pond to have them reflected properly at runtime. Kato's baseUrl can be configured using the +kato.baseUrl+ key, and likewise for Oort using the +oort.baseUrl+ key. These must be fully qualified (scheme and all) URLs. They can be passed at startup time to the Pond executable as Java system properties in the form +JAVA_OPTS='-Dkato.baseUrl=http://kato.url:port -Doort.baseUrl=http://oort.url:port' ./pond+.

== Bootstrapping the Deployment UI

Spinnaker's deployment UI, Deck, provides a web interface for managing cloud deployments. Its user experience is built on the mantra of "context over console", where a goal of the application is to provide detail at appropriate levels of opinionated disclosure and operations within context. This experience allows users to retain "spatial awareness" as they interrogate the presence of their applications in the cloud, and to act within the confines of a deliberate context, as opposed to having to context switch to perform multiple operations.

=== Building Deck

Deck is a Angular application, which is first built with Grunt, then packaged into a webjar with Gradle. It is distributed, then later consumed by Amps, which provides a container for the static content to be served. This process is not entirely necessary, however, if the static content can be hosted on CDN or in a static Apache folder. To build the Deck distribution, NPM and bower must be installed on the build server. The project can be staged for build by running +npm install+, which will pull down the necessary dependencies. Following that, a +grunt build+ command will compile the project into the +dist+ directory of the project.

To generate a webjar of the project, after it has been built with Grunt, the Gradle build can be invoked in the form of +./gradlew clean build+, which will place the static assets in the +META-INF/resources+ directory of the generated artifact.

=== Running Deck

Since Deck generates down to a collection of static assets, it can be released to any HTTP capable web server for deployment. From a development perspective, however, the project can be run using the +grunt serve+ command.

=== Configuring Deck

As of the time of this writing, Deck has hardcoded URLs for Spinnaker's deployment orchestration engine, its cluster service, and its infrastructure service. These need to be manually adapted to the target environment by editing the +app/scripts/settings/settings.js+ script.

[[CONF]]
== Additional Configuration of Spinnaker Services

All Spinnaker services have the ability to be configured to manage any number of accounts, regions, and availability zone configurations. In their simplest form, however, they require no additional configuration beyond allowing the AWS SDK to be able to reach credentials for a target account. Any of the possibilities from the http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/credentials.html[AWS documentation] will allow these service to be able to reach a single, default account, which is all that's necessary for them to run in a standalone capacity. Additional accounts can be configured and used through http://docs.aws.amazon.com/IAM/latest/UserGuide/roles-assume-role.html[assume role configuration]. The source account still needs to be configured as described, however credentials for the target accounts need not be known to Spinnaker's services. The only thing that needs to be known the target account ID and the role that must be assumed by the caller. Spinnaker services are also capable of being configured to make use of https://github.com/netflix/edda[Edda] instead of calling AWS directly for some collections. Spinnaker can also integrate with https://github.com/netflix/eureka[Eureka] for health checks and to put instances "out of service" during deployment related operations. In the case of Eureka, its configuration is described by "Discovery", which is how it is known internal to Netflix.

```
aws:                                                                                                                                                    
  accounts:
    - name: test
      edda: http://edda.%s.test.netflix.net
      discovery: http://%s.dtest.netflix.net
      accountId: 1234
      assumeRole: role/asgard
      regions:
        - name: us-east-1
          availabilityZones:
          - us-east-1a
          - us-east-1b
          - us-east-1c
          - us-east-1d
          - us-east-1e
    - name: prod
      edda: http://edda.%s.prod.netflix.net
      discovery: http://%s.dprod.netflix.net
      accountId: 1234
      assumeRole: role/asgard
      regions:
        - name: us-east-1
          availabilityZones:
          - us-east-1a
          - us-east-1b
          - us-east-1c
          - us-east-1d
          - us-east-1e
```

The values for Edda and Eureka are in string replacement format, where the +%s+ will be replaced by the region in which the operation is presenting occurring. For example, if the cluster service were looking for all clusters in the +us-east-1+ region for the **test** account, and it intended to go through Edda for that call, the URL above would be mapped to +http://edda.us-east-1.test.netflix.net+.
